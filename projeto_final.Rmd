---
title: "\\begin{center}	\\textbf{UNIVERSIDADE FEDERAL DO RIO DE JANEIRO}\\\\ Departamento de Métodos Estatísticos - IM\\\\ Estatística computacional\\\\\\vspace{250 pt}\\textbf{{\\Large TÍTULO}}\\\\ Projeto final\\\\ \\vspace{160 pt}\\begin{flushright}\\textbf{Aluno:} Silvaneo Vieira dos Santos Junior\\\\ \\textbf{Professor:} Carlos Tadeu Pagani Zanini\\\\ \\end{flushright}\\end{center}"
author: ""
output: 
  pdf_document: 
    toc: yes
    number_sections: yes
toc-title: "ÍNDICE"
date: "`r Sys.setlocale('LC_TIME', 'Portuguese'); format(Sys.Date(),'%d de %B de %Y')`"
bibliography: references.bib 
link-citations: yes
header-include:
  - \usepackage{cancel}
  - \usepackage[portuguese]{babel}
  - \usepackage{natbib}
  - \bibliographystyle{#1}
  - \bibliography{#2}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_TIME", "Portuguese")

library(tidyverse)
```

\newpage

# Introdução

\pagebreak

# Metodologia

Neste trabalho estaremos interessados em avaliar o ajuste de Modelos Lineares Generalizados (GLM) usando uma variação da metodologia proposta em @AlvesGDLM. Nesta sessão apresentaremos a base da metodologia utilizada, começando por uma breve introdução aos GLM, em seguida apresentaremos uma versão simplificada do Teorema da Projeção [@AmariGeomInfo] e a forma como aplicaremos esse teorema para o problema que desejamos resolver. Finalizamos essa sessão com algumas considerações sobre algumas formas de se calcular as integrais necessárias para a aplicação da metodologia.

## Modelos Lineares Generalizados

A classe dos Modelos Lineares Generalizados [@McCulloch2001; @dobson2018] é uma classe de modelos muito ampla e que generaliza os Modelos Lineares com resposta Normal. De modo geral, vamos assumir que temos um conjunto de observações $Y_{i}$, tais que:

\begin{equation}
\label{GLMDef}
\begin{aligned}
Y_{i}|\theta &\sim F(\lambda_i),\\
g(\lambda_i) &= \eta = x'_{i}\theta,
\end{aligned}
\end{equation}

onde $g$, chamada função de ligação, é uma função contínua e monótona, $\eta$ é o preditor linear, $X$ é a matriz de planejamento, $x_{i}$ é a $i$-ésima coluna de $X$, $\theta$ (possivelmente um vetor) são os parâmetros latentes que representam o efeito das variáveis explicativas e $F$ representa uma distribuição pertencente a família exponencial e indexada pelo parâmetro $\lambda_i$ (possivelmente um vetor). Nesse trabalho sempre vamos considerar que $g$ e $X$ são conhecidos.

Sendo $F$ pertencente a família exponencial, então temos que, por definição, podemos escrever a densidade de $Y_i|\theta$ como:

\begin{equation}
\label{ExpFamDef}
\begin{aligned}
f(y_{i}|\theta) &= \exp\left\{\lambda_i \cdot H(y_i) - A(\lambda_i) + B(y_{i}) \right\},
\end{aligned}
\end{equation} onde as funções $H$, $A$ e $B$ são conhecidas. Neste trabalho o vetor $H(y_i)$ será chamado de vetor de estatísticas suficientes.

Adiante apresentamos um importante resultado associado as distribuições pertencentes à família exponencial [@migon2014statistical]:

\textbf{Teorema 1:} \emph{
Seja $Y_i$ com distribuição pertencente à família exponencial conforme (\ref{ExpFamDef}), então vale que:
\begin{equation}
\label{Teom1}
\begin{aligned}
\mathbb{E}[H_j(y_i)] &= \frac{\partial A(\lambda_i)}{\partial \lambda_{ij}}.
\end{aligned}
\end{equation}
}

Alguns exemplos de distribuições pertencentes à família exponencial são: Normal com média e variância desconhecidas, Gamma com parâmetro de forma e escala desconhecidos, Beta, Multinomial com parâmetro de tentativas conhecido, Binomial Negativa com número de fracassos conhecido, Poisson, Geométrica, Rayleigh, Pareto com locação conhecida e Pareto Assimétrica com locação conhecida. Neste trabalho estaremos especialmente interessados nos casos Normal com média e variância desconhecidas, Rayleigh e Pareto Assimétrica com locação conhecida.

Abordaremos neste trabalho como realizar a análise Bayesiana para dados provenientes de um modelo observacional conforme especificado em (\ref{GLMDef}). Para isso, vamos especificar uma priori $\pi_0$ para $\theta$ e, para realizar qualquer que seja a análise, devemos encontrar a distribuição a posteriori de $\theta$ ($\pi_n$). Como pode ser visto em @migon2014statistical, podemos escrever $\pi_n$ como:

\begin{equation}
\label{Post-def}
\begin{aligned}
\pi_n(\theta|y_1,...,y_n) = \frac{\pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta)}{\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta}.
\end{aligned}
\end{equation}

Infelizmente, a menos do caso onde $F$ representa a distribuição Normal com varância conhecida, não é possível obter uma solução analítica para $\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta$, sendo necessário recorrer a métodos de integração numérica para obter $\pi_n(\theta|y_1,...,y_n)$ ou estimativas de $\theta$. Tendo em mente que métodos de integração numérica podem ter um custo computacional muito elevado, especialmente quando a dimensão de $\theta$ é grande, @AlvesGDLM propõe o uso de aproximações para $\pi_n$ de modo que possamos obter uma forma analítica aproximada para a posteriori de $\theta$. No trabalho original, a metodologia é apresentada para Modelos Dinâmicos Lineares Generalizados, porém os GLM são um caso particular desta classe, de modo que a metodologia proposta também é válida para os modelos em que estamos interessados neste trabalho, contudo, usaremos uma versão modificada desta metodologia, de modo que não discutiremos o trabalho original de @AlvesGDLM, mas apresentaremos diretamente a versão modificada (que é mais simples).

## Inferência aproximada via Teorema da Projeção

Como visto na sessão anterior, para realizar o processo de inferência, precisamos obter $\pi_n(\theta|y_1,...,y_n) = \frac{\pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta)}{\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta}$, sendo que nem sempre podemos obter uma forma analítica fechada para $\pi_n$. Seguindo a abordagem proposta em @AlvesGDLM, para solucionar este problema, iremos aproximar $\pi_n$ por uma $\widehat{\pi}_n$ que seja próxima da posteriori verdadeira, mas que tenha propriedades úteis, por exemplo, tal que $\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta$ seja tratável. Para a escolha de $\widehat{\pi}_n$, vamos escolher, dentro de uma família de distribuições, aquela que minimiza a divergência de Kullback-Leibler (KL), definida como:

\begin{equation}
\label{KL-def}
KL(p||q) = \mathbb{E}_p[\ln(f_q(X))-\ln(f_p(X))],
\end{equation}
onde $p$ e $q$ são distribuições de probabilidade, $f_p$ e $f_q$ são funções de densidade ou de massa de probabilidade associadas, respectivamente, a $p$ e $q$ e $\mathbb{E}_p$ representa o valor esperado calculado considerando que $X$ tem distribuição $p$.

Para os fins deste trabalho, é natural escolher $\widehat{\pi}_n$ tal que $KL(\pi_n,\widehat{\pi}_n)$ é minimal (ver capítulo 5 de @infoTeoMacKay para uma interpretação intuitiva da divergência KL). Por conveniência, vamos escolher $\widehat{\pi}_n$ como pertencente à família Normal, de modo que, dado $\pi_n$, basta encontrar os parâmetros de média e variância para $\widehat{\pi}_n$ tais que $KL(\pi_n,\widehat{\pi}_n)$ é otimal. Para auxiliar no processo de encontrar os parâmetros ótimos de $\widehat{\pi}_n$, podemos usar o seguinte teorema (ver @AmariGeomInfo):

\textbf{Teorema 2 (da Projeção):} \emph{
Seja $p$ e $q$ duas distribuições de variáveis aleatórias contínuas \footnote{O Teorema ainda vale sem essa restrição} pertencentes à família exponencial, conforme (\ref{ExpFamDef}), isto é, existem densidades de probabilidade $f_p$ e $f_q$ associadas, respectivamente, a $p$ e $q$ tais que:

\begin{equation}
\label{formTeo2}
f_p(x)=\exp\left\{\lambda_p \cdot H_p(x) - A_p(\lambda_p) + B_p(x) \right\}\\
f_q(x)=\exp\left\{\lambda_q \cdot H_q(x) - A_q(\lambda_q) + B_q(x) \right\}.
\end{equation}

Então, fixados os parâmetros $\lambda_p$ associados à distribuição $p$, os parâmetros $\lambda_q$ da distribuição $q$ que minimizam a divergência KL são únicos (quando existem) e satisfazem o seguinte sistema:

\begin{equation}
\label{Teom2}
\mathbb{E}_p[H_q]=\mathbb{E}_q[H_q].
\end{equation}

Vale ainda que, se existe $\lambda_q$ que satisfaz o sistema acima, então existe um mínimo para a divergência KL com relação a $\lambda_q$ e a solução para o sistema \ref{Teom2} é única.
}

Apresentaremos a prova parcial do Teorema 2, pois a enunciação do Teorema da Projeção apresentada em @AmariGeomInfo é muito mais geral do que o teorema que usaremos. Vamos nos limitar a provar que os parâmetros $\lambda_q$ que minimizam a divergência KL satisfazem \ref{Teom2}, sendo que os argumentos para a existência e unicidade do mínimo e da unicidade da solução do sistema podem ser encontrardo em @AmariGeomInfo.

Para provar o Teorema 2, primeiro observe que a divergência KL de $p$ com relação a $q$ pode ser escrita como:

\begin{equation}
\label{formTeo2}
\begin{aligned}
KL(p||q)&=\mathbb{E}_p[\lambda_q \cdot H_q(X) - A_q(\lambda_q) + B_q(X)  - (\lambda_p \cdot H_p(X) - A_p(\lambda_p) + B_p(X))])\\
&=\lambda_q \cdot \mathbb{E}_p[H_q(X)] - A_q(\lambda_q) + \mathbb{E}_p[B_q(X)]  - \lambda_p \cdot \mathbb{E}_p[H_p(x)] + A_p(\lambda_p) - \mathbb{E}_p[B_p(X))].
\end{aligned}
\end{equation}

Veja que, se $\lambda_q$ minimiza $KL(p||q)$, usando propriedades da divergência KL na família exponencial (i.e., que ela é contínua e duas vezes diferenciável em relação à $\lambda_q$), temos que:

\begin{equation}
\label{sistem1Teo2}
\frac{\partial}{\partial \lambda_{qi}}KL(p||q)=0, \forall i.
\end{equation}

Mas veja que:

\begin{equation}
\label{derivTeo2}
\frac{\partial}{\partial \lambda_{qi}}KL(p||q)=\mathbb{E}_p[H_{qi}(X)] - \frac{\partial}{\partial lambda_{qi}}A_q(\lambda_q),
\end{equation}
pois os valores esperados com relação a $p$ não dependem de $\lambda_q$.

Daí obtemos que, se $\lambda_q$ minimiza $KL(p||q)$, então:

\begin{equation}
\label{sistem1Teo2}
\mathbb{E}_p[H_{qi}(X)] = \frac{\partial}{\partial \lambda_{qi}}A_q(\lambda_q), \forall i.
\end{equation}

Usando a equação (\ref{Teom1}) do Teorema 1 e lembrando que a equação acima deve valer para todas as coordenadas de $\lambda_q$, obtemos que:

$$
\mathbb{E}_p[H_{q}(X)] = \mathbb{E}_q[H_{q}(X)].
$$

E isso conclui a prova parcial do Teorema 2. $\square$

No caso específico em que estamos trabalhando, vamos tomar $q$ como pertencente à família Normal (em alguns casos, multivariada), de modo que:

\begin{equation}
\label{HqDef}
H_{q}(X)=(X,XX')'.
\end{equation}

Na prática, isso significa que a $q$ que melhor aproxima $p$ é aquela que tem o mesmo vetor de médias e a mesma matriz de covariância.

A princípio, a metodologia descrita até o momento parece bastante simples: Basta aproximar $\pi_n$ por $\widehat{\pi}_n$ e realizar toda a inferência com base nesta última distribuição que, pertencendo à família Normal, é bem fácil de se trabalhar. Porém, talvez o leitor tenha observado que há uma certa incosistência na metodologia: Não podemos calcular $\pi_n$ ,pois não temos forma analítica fechada para $\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta$, então iremos aproximar a posteriori verdadeira por $\widehat{\pi}_n$, sendo que, para isto, devemos calcular 
$\mathbb{E}_p[H_{q}(X)]$. Ora, se não conseguimos calcular $\int \pi_0(\theta) \prod_{i=1}^{n} f(y_i|\theta) d\theta$, não é razoável assumir que conseguimos calcular $\mathbb{E}_p[H_{q}(X)]$! De certa forma, estaríamos trocando um problema por outro de igual complexidade (se não maior!).

Felizmente, trabalhar com $\mathbb{E}_p[H_{q}(X)]$ não é tão problemático quanto trabalhar diretamente com $\pi_n$, pois precisamos apenas do valor de $\mathbb{E}_p[H_{q}(X)]$, e não de uma expressão analítica para ele, Desta forma, podemos calcular $\mathbb{E}_p[H_{q}(X)]$ usando métodos de integração numérica, especificamente, vamos trabalhar com 4 métodos para calcular os valores esperados desejados: Quadratura Gaussiana, Aproximação de Laplace, Monte Carlo e pelo método proposto em @TierneyKadane1 e refinado em TierneyKadane2. Usaremos Quadratura Gaussiana nos Casos Normal com variância desconhecida e Rayleight, pois nestes casos temos de lidar com integrais univariadas, de modo que o custo computacional de se usar Quadratura Gaussiana é despresível. Para o caso Laplace Assimétrica, usaremos os outros métodos afim de conseguir um ajuste satisfatório.

\pagebreak

# Qualidade da aproximação

Como visto na sessão anterior, o Teorema 2 no fornece uma forma simples de aproximar uma distribuição de probabilidade por outra, desde que ambas pertençam à família exponencial, especificamente, esse teorema nos diz qual é **a melhor** distribuição que aproxima nossa posteriori. Naturalmente, **a melhor** distribuição pode não ser boa, por isso, é necessário alguma investigação sobre o assunto. Felizmente, graças as propriedades da família exponencial (ver @AmariGeomInfo e, para o caso mais geral, @TierneyKadane1 e @migon2014statistical), temos que a aproximação será muito boa desde que o tamanho da amostra seja suficientemente grande.

Para exemplificar o comportamento descrito acima, vamos exibir adiante uma comparação entre as posterioris aproximadas para vários tamanhos de amostra. Neste caso vamos supor um modelo muito simples:

\begin{equation}
\label{Model1}
Y_i|\theta \sim Poisson(\lambda)\\,
\ln(\lambda)= \theta\\,
\theta \sim \mathcal{N}(0,100)\\,
\end{equation}
onde tomaremos $\lambda=1$ para gerar os dados.


```{r}
set.seed(13031998)

data_plot=data.frame()

for(n in c(1,5,10,20)){
y=rpois(n,1)
y_stat=sum(y)

f=function(x){exp(y_stat*x-n*exp(x)-(x**2)/(2*100))}
c=integrate(f,-Inf,Inf)$value
x_mean=integrate(function(x){x*f(x)},-Inf,Inf)$value/c
x2_mean=integrate(function(x){(x**2)*f(x)},-Inf,Inf)$value/c
s2=x2_mean-x_mean**2

x=seq(x_mean-3*sqrt(s2),x_mean+3*sqrt(s2),l=1000)

data_plot=rbind(data_plot,data.frame(x=x,fx=f(x)/c,gx=dnorm(x,x_mean,sqrt(s2)),n=n))
}

data_plot$n=factor(paste0('Tamanho da amostra:',data_plot$n),levels=paste0('Tamanho da amostra:',c(1,5,10,20)))

ggplot(data_plot)+
  geom_line(aes(x=x,y=fx,linetype='Dist. exata'))+
  geom_line(aes(x=x,y=gx,linetype='Dist. aprox'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
  theme_bw()+
  facet_wrap(.~n,scales = 'free')


```

Veja que temos uma aproximação com qualidade muito boa, mesmo para amostras relativamente pequenas. Vale observar que, no caso da distribuição Poisson, a qualidade da aproximação dependende da magnitude dos dados observados. De modo geral, a qualidade vai depender da quantidade de informação na amostra, sendo que é fácil ver (via, por exemplo, a informação de Fisher) observações de valores maiores tem muito mais informação do que obserções de valores menores (especialmente o $0$). O caso que mostramos anteriormente seria uma caso **ruim**, pois a taxa verdadeira da Poisson (isto é, a taxa usada para gerar os dados) foi igual a 1. Adiante, vamos mostrar o mesmo exemplo, mas agora gerando dados de uma Poisson com taxa 5 (que ainda é um valor relativamente baixo):

```{r}
set.seed(13031998)

data_plot=data.frame()

for(n in c(1,5,10,20)){
y=rpois(n,5)
y_stat=sum(y)

f=function(x){exp(y_stat*x-n*exp(x)-(x**2)/(2*100))}
c=integrate(f,-Inf,Inf)$value
x_mean=integrate(function(x){x*f(x)},-Inf,Inf)$value/c
x2_mean=integrate(function(x){(x**2)*f(x)},-Inf,Inf)$value/c
s2=x2_mean-x_mean**2

x=seq(x_mean-3*sqrt(s2),x_mean+3*sqrt(s2),l=1000)

data_plot=rbind(data_plot,data.frame(x=x,fx=f(x)/c,gx=dnorm(x,x_mean,sqrt(s2)),n=n))
}

data_plot$n=factor(paste0('Tamanho da amostra:',data_plot$n),levels=paste0('Tamanho da amostra:',c(1,5,10,20)))

ggplot(data_plot)+
  geom_line(aes(x=x,y=fx,linetype='Dist. exata'))+
  geom_line(aes(x=x,y=gx,linetype='Dist. aprox.'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
  theme_bw()+
  facet_wrap(.~n,scales = 'free')


```

Observe que, agora, a aproximação é razoável mesmo para uma amostra com apenas 1 elemento, sendo que ela é praticamente idêntica à distribuição exata para amostras de tamanho maior que 10.

O modelo descrito em (\ref{Model1}) é útil para exemplificar propriedades gerais da aproximação, porém ele não representa bem o tipo de modelo que gostaríamos de ajustar em problemas reais, uma vez que, nesta especificação, as observações $y_i$ são i.i.d.. De modo geral, estaremos interessados em um modelo da forma descrita em (\ref{GLMDef}), onde teremos um conjunto de covariáveis das quais queremos estimar o efeito. No exemplo apresentado tivemos de lidar apenas com uma variável, de modo que as integrais a serem calculadas eram univariadas, o que permitiu o uso de quadratura Gaussiana com um custo computacional desprezível. Ao lidar com um conjunto de variáveis as integrais com as devemos trabalhar passam a ser multivariadas, o que torna o uso de métodos de integração determinísticos inviável (para um conjunto grande de parâmetros no modelo).

Felizmente, há uma solução para o problema mencionado acima. Primeiro, para apresentar essa proposta, suponha que há apenas uma observação, de modo que nossa posteriori é propocional à $f(y_1|\theta)\pi_0(\theta)$. Veja que, no nosso modelo, $\theta$ depende de $y_1$ apenas através do preditor linear $\eta_1$, que por sua vez é univariado. Apartir da priori Normal de $\theta$, temos uma priori Normal para $\eta_1$ e podemos obter a posteriori aproximada para $\eta_1$ usando a metodologia descrita anteriormente (como $\eta_1$ é sempre univariado, temos que as integrais podem ser resolvidas facilmente com métodos numéricos determinísticos). Uma vez obtida a posteriori Normal para $\eta_1$, é fácil obter a posteriori para $\theta$ (mesma fórmula usada em Modelos Dinâmicos Lineares, ver @PetrisDLM, @WestHarrDLM ou @Kalman_filter_origins):

\begin{equation}
\label{post_theta}
\begin{aligned}
\eta_1|y_1 \sim \mathcal{N}(\mu,\sigma^2) \quad \theta\sim \mathcal{N}(\vec{m}_0,V_0) \rightarrow \theta|y_1 &\sim \mathcal{N}(\vec{m}_1,V_1),\\
\vec{m}_1 &= \vec{m}_0 + V_0x_1(x_1'V_0x_1)^{-1}(\mu-x_1'\vec{m}_0),\\
V_1 &= V_0 + V_0x_1(x_1'V_0x_1)^{-1}(\sigma^2-x_1'V_0x_1)(x_1'V_0x_1)^{-1}x_1'V_0.
\end{aligned}
\end{equation}

Com as equações acima podemos atualizar a distribuição de $\theta$ de forma computacionalmente eficiente, independente da dimensão de $\theta$. Infelizmente, isso só resolve o nosso problema para amostras de tamanho unitário, de fato, para uma amostra de tamanho $n$ qualquer, devemos obter a posteriori conjunta dos $n$ preditores lineares para podermos usar a fórmula (\ref{post_theta}), para isso deveríamos resolver algumas integrais $n$ variadas, ou seja, o problema inicial ainda persiste. Resta então apresentar um último resultado teórico que finalmente vai nos permitir fugir da "maldição da dimensionalidade". Veja que, em geral, vale que:

\begin{equation}
\label{prop_decomp}
\begin{aligned}
f(\theta|y_1,...,y_n) \propto f(y_n|\theta, y_1,...,y_{n-1})f(y_{n-1}|\theta, y_1,...,y_{n-2}) \cdots f(y_2|\theta, y_1)f(y_1|\theta)f(\theta).
\end{aligned}
\end{equation}

Usando que, por hipótese, os $y_i$'s são independentes dado $\theta$, podemos simplificar a equação acima da seguinte forma:

\begin{equation}
\label{prop_decomp2}
\begin{aligned}
f(\theta|y_1,...,y_n) &\propto f(y_n|\theta)f(y_{n-1}|\theta) \cdots f(y_2|\theta)f(y_1|\theta)f(\theta),\\
=f(\theta)\prod_{i=1}^{n}f(y_i|\theta).
\end{aligned}
\end{equation}

Observe que $f(y_1|\theta)f(\theta) \propto f(\theta|y_1)$, de modo que podemos obter uma forma aproximada para $f(y_1|\theta)f(\theta)$ usando a metodologida proposta. Com isto, podemos escrever $f(y_2|\theta)f(y_1|\theta)f(\theta) \propto f(y_2|\theta, y_1)f(\theta|y_1)$ e, interpretando $f(\theta|y_1)$ como uma nova priori para $\theta$, podemos usar a metodologia proposta para obter uma forma aproximada para $f(\theta|y_1,y_2)$. Repetindo esse processo elemente a elemento da amostra podemos obter uma forma aproximaa para $f(\theta|y_1,...,y_n)$ usando apenas integrais univariadas. Para efeito de comparação, digamos que, ao usar Quadratura Gaussiana para resolver uma integral, devemos avaliar a função $f(y_1|\theta)f(\theta)$ $k$ vezes para cada dimensão, de modo que, se fóssemos calcular as integrais diretamente, deveríamos avaliar a função $k^n$ (teríamos uma malha $n$ dimensional e repartiríamos cada dimensão em $k$ segmentos), no caso da integração pelos preditores lineares, ou $k^r$, no caso da integração diretamente nos parâmetros latentes $\theta$, em contrapartida, usando a aproximação elemento a elemento da amostra, devemos avaliar a função $f(y_1|\theta)f(\theta)$ apenas $k \times n$ vezes!

Claramente essa proposta reduz dráticamente a quantidade de vezes que devemos computar a função $f(y_1|\theta)f(\theta)$, porém há um preço a se pegar por essa simplificação: Ao usar a distribuição aproximada diversas vezes, há um acumulo do erro de aproximação. Sendo assim, devemos avaliar se, usando esse método simplificado, não estamos tendo perdas significativas na qualidade da aproximação.

```{r}
set.seed(13031998)

data_plot=data.frame()

for(n in c(1,5,10,20)){
y=rpois(n,5)
y_stat=sum(y)

f=function(x){exp(y_stat*x-n*exp(x)-(x**2)/(2*100))}
c=integrate(f,-Inf,Inf)$value
x_mean=integrate(function(x){x*f(x)},-Inf,Inf)$value/c
x2_mean=integrate(function(x){(x**2)*f(x)},-Inf,Inf)$value/c
s2=x2_mean-x_mean**2

x_mean2=0
s22=100
for(i in 1:n){
  f=function(x){exp(y[i]*x-exp(x)-((x-x_mean2)**2)/(2*s22))}
  c_prev=integrate(f,-Inf,Inf)$value
  x_mean_prev=integrate(function(x){x*f(x)},-Inf,Inf)$value/c_prev
  x2_mean_prev=integrate(function(x){(x**2)*f(x)},-Inf,Inf)$value/c_prev
  s2_prev=x2_mean_prev-x_mean_prev**2
  
  x_mean2=x_mean_prev
  s22=s2_prev
  print(s22)
}

f=function(x){exp(y_stat*x-n*exp(x)-(x**2)/(2*100))}
x=seq(x_mean-3*sqrt(s2),x_mean+3*sqrt(s2),l=1000)

data_plot=rbind(data_plot,data.frame(x=x,
                                     fx=f(x)/c,
                                     gx=dnorm(x,x_mean,sqrt(s2)),
                                     hx=dnorm(x,x_mean2,sqrt(s22)),
                                     n=n))
}

data_plot$n=factor(paste0('Tamanho da amostra:',data_plot$n),levels=paste0('Tamanho da amostra:',c(1,5,10,20)))

ggplot(data_plot)+
  geom_line(aes(x=x,y=fx,linetype='Dist. exata'))+
  geom_line(aes(x=x,y=gx,linetype='Dist. aprox.',color='Ori.'))+
  geom_line(aes(x=x,y=hx,linetype='Dist. aprox.',color='Alt.'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
  scale_color_manual('',values=c('red','blue'))+
  theme_bw()+
  facet_wrap(.~n,scales = 'free')


```

Veja que o método alternativo introduz, de fato, um erro adicional significativo, mas que é, contudo, tolerável, uma vez que a redução do custo computacional é muito grande e, como veremos na próxima sessão, em termos práticos, a diferença entre a posteriori exata e a posteriori aproximada pelo método alternativo é desprezível.

\pagebreak

# Aplicações

Nesta sessão apresentaremos três aplicações do método proposto 

## Caso Normal com variância desconhecida

## Caso Rayleight

## Caso Laplace Assimétrica

\pagebreak

# Conclussões

\pagebreak

# Referências
