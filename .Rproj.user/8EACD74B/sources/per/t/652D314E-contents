---
title: "Modelos Lineares Generalizados"
subtitle: "Lista 3"
author: "Silvaneo Viera dos Santos Junior"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: references.bib 
header-include:
  - usepackage(cancel)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\def\P{\mathbb{P}}

Exercícios 3 a 7 do capítulos 8 de @McCulloch2001.

# Questão 3

Seja $y_{ij}$ e $\vec{u}$ tais que:

\begin{equation}
\label{8.15}
\begin{aligned}
y_{ij}|\vec{u} &\sim Poisson(\mu_{ij})\\
\ln \mu_{ij}& = x'_{ij}\beta+u_i\\
u_i & \sim \mathcal{N}(0,\sigma^2_u),
\end{aligned}
\end{equation}

com os $u_i$'s independentes entre si e os $y_{ij}$'s independentes dado $\vec{u}$. Com isso, é fácil ver que a distribuição conjunta de dos $y_{ij}$'s e dos $u_i$'s é tal que:

\begin{equation}
\label{raw_conj_y_u}
\begin{aligned}
f(\vec{y},\vec{u})&=f(\vec{y}|\vec{u})f(\vec{u})=\prod_{i}f(u_i)\prod_{j}f(y_{ij}|u_i)\\
                  &=\prod_{i}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\prod_{j}\frac{\mu_{ij}^{y_{ij}}}{y_{ij}!}\exp\{-\mu_{ij}\}\\
                  &=\prod_{i}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\prod_{j}\frac{1}{y_{ij}!}\exp\left\{y_{ij}x'_{ij}\beta+ y_{ij}u_i +\exp\{-x'_{ij}\beta-u_i\}\right\},
\end{aligned}
\end{equation}
onde $\vec{y}$ é um vetor de inteiros não negativos.

Para obter a distrição marginal de $\vec{y}$ basta integrar \ref{raw_conj_y_u} com relação a $\vec{u}$, de modo que:

\begin{equation}
\label{raw_conj_y}
\begin{aligned}
f(\vec{y})&=\int_{\mathbb{R}^{m}}(\vec{y},\vec{u})d\vec{u}
           =\int_{\mathbb{R}^{m}}\prod_{i}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\prod_{j=1}^{n_i}\frac{1}{y_{ij}!}\exp\left\{y_{ij}x'_{ij}\beta+ y_{ij}u_i +\exp\{-x'_{ij}\beta-u_i\}\right\}d\vec{u}\\
          &=\prod_{i,j}\frac{1}{y_{ij}!}\exp\left\{y_{ij}x'_{ij}\beta\right\} \times \int_{\mathbb{R}^{m}}\prod_{i}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\exp\left\{\sum_{j}y_{i}u_i +\sum_{j}\exp\{-x'_{ij}\beta\}\exp\{-u_i\}\right\}d\vec{u}\\
          &=\prod_{i,j}\frac{1}{y_{ij}!}\exp\left\{y_{ij}x'_{ij}\beta\right\} \times \prod_{i}\int_{\mathbb{R}}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\exp\left\{\sum_{j}y_{i}u_i +\sum_{j}\exp\{-x'_{ij}\beta\}\exp\{-u_i\}\right\}du_i,
\end{aligned}
\end{equation}
sendo que a última igualdade é verdadeira devido a independência entre os $u_i$'s.

Por último, aplicando a função logarítmo em \ref{raw_conj_y}, obtemos:

\begin{equation}
\label{conj_y}
\begin{aligned}
\ln f(\vec{y})&=-\sum{i,j}\ln(y_{ij}!) + \vec{y}X\beta\\  
              & + \sum_{i} \ln \int_{\mathbb{R}}\frac{1}{\sqrt{2\pi\sigma^{2}_u}}\exp\left\{-\frac{u_i^2}{2\sigma^{2}_u}\right\}\exp\left\{\sum_{j}y_{i}u_i +\sum_{j}\exp\{-x'_{ij}\beta\}\exp\{-u_i\}\right\}du_i.
\end{aligned}
\end{equation}

É fácil ver que, a menos de pequenas diferenças de notação, a equação \ref{conj_y} equivale ao que é pedido pelo enunciado.

# Questão 4

Veja que, em geral, para um par de variáveis aleatórias $Y,\delta$ quaisquer e $A$ um subconjunto de $\mathbb{R}$, vale, pela Lei da Probabilidade total que:

\begin{equation}
\label{tot_prob_law}
\begin{aligned}
\P(Y>k)=\P(Y>k|\delta \in A)\P(\delta \in A)+\P(Y>k|\delta \notin A)\P(\delta \notin A)
\end{aligned}
\end{equation}

No caso particular em que $Y=\delta X$, $X \sim \chi^2_1$, $\P(\delta=0)=\P(\delta=1)=\frac{1}{2}$ e $A={1}$, temos que, para todo $k$ positivo:

\begin{equation}
\label{p_k}
\begin{aligned}
\P(Y>k)=\P(Y>k|\delta=1)\P(delta=1)+\P(Y>k|\delta=0)\P(\delta=0)=\frac{1}{2}\P(X>k),
\end{aligned}
\end{equation}
pois $\P(Y>k|\delta=0)=0$ para todo $k>0$. Tomando $k=\chi^2_{1,1-2\alpha}$, então, por definição, $\P(X>k)=2\alpha$, daí:

$$
\P(Y>\chi^2_{1,1-2\alpha})=\alpha.
$$

# Questão 5

Neste modelo vamos assumir que:

\begin{equation}
\label{Model-8.6d}
\begin{aligned}
y_{ij}|\lambda_{ij} &\sim Poisson(\lambda_{ij})\\
\lambda_{ij}&\sim Gamma(\gamma_i,\beta_i).
\end{aligned}
\end{equation}

Vale que a distribuição conjunta de $y_{ij},\lambda_{ij}$ é tal que:

\begin{equation}
\label{raw_y_lambda}
\begin{aligned}
f(y_{ij},\lambda_{ij}) &= f(y_{ij}|\lambda_{ij})f(\lambda_{ij}) \\
                       &= \frac{\lambda_{ij}^{y_{ij}}}{y_{ij}!}e^{-\lambda_{ij}}\frac{\beta_i^{\gamma_i}}{\Gamma(\gamma_i)}\lambda_{ij}^{\gamma_i-1}e^{-\beta_i\lambda_{ij}}\\\\
                       &= \frac{1}{y_{ij}!}\frac{\beta_i^{\gamma_i}}{\Gamma(\gamma_i)}\lambda_{ij}^{y_{ij}+\gamma_i-1}e^{-(\beta_i+1)\lambda_{ij}}.
\end{aligned}
\end{equation}

Daí, obtemos:

\begin{equation}
\label{y_densi}
\begin{aligned}
f(y_{ij}) &= \int_{0}^{+\infty}f(y_{i,j},\lambda_{ij})d\lambda_{ij} \\
                       &= \frac{1}{y_{ij}!}\frac{\beta_i^{\gamma_i}}{\Gamma(\gamma_i)}\int_{0}^{+\infty}\lambda_{ij}^{y_{ij}+\gamma_i-1}e^{-(\beta_i+1)\lambda_{ij}}d\lambda_{ij}\\
                       &= \frac{1}{y_{ij}!}\frac{\beta_i^{\gamma_i}}{\Gamma(\gamma_i)}\frac{\Gamma(y_{ij}+\gamma_i)}{(\beta_i+1)^{y_{ij}+\gamma_i}}\\
                       &= \begin{pmatrix}y_{ij}+\gamma_i-1\\ y_{ij}\end{pmatrix}\left(\frac{\beta_i}{\beta_i+1}\right)^{\gamma_i}\left(\frac{1}{\beta_i+1}\right)^{y_{ij}}.
\end{aligned}
\end{equation}

É fácil ver pela equação \ref{y_densi} que $y_ij$ tem distribuição Binomial Negativa com parâmetros $\gamma_i,\frac{1}{\beta_i+1}$. Usando que os $\lambda_{ij}$'s são independentes e que os $y_{ij}$'s são independentes dado os $\lambda_{ij}$'s, temos que:

\begin{equation}
\label{log_like_ex5}
\begin{aligned}
\ln f(y_{ij}) &= \sum_i \sum_j \ln(\Gamma(y_{ij}+\gamma_i)) - \ln(y_{ij}!) - \ln(\Gamma(\gamma_i)) +\gamma_i\ln\left(\frac{\beta_i}{\beta_i+1}\right) + y_{ij}\ln\left(\frac{1}{\beta_i+1}\right).
\end{aligned}
\end{equation}

# Questão 6

É fácil ver que a log-verossimilhança de $\beta$ dado as observações ($l$) pode ser escrita como:

\begin{equation}
\label{log_like_ex6}
\begin{aligned}
l(\beta,\vec{u}|y_{ij}) &= \sum_{i} y_{i1}\ln(\pi_{i1})+(1-y_{i1})\ln(1-\pi_{i1})+y_{i2}\ln(\pi_{i2})+(1-y_{i2})\ln(1-\pi_{i2}).
\end{aligned}
\end{equation}

Derivando com relação a $u_i$ obtemos:

\begin{equation}
\label{log_like_ex6_part_u}
\begin{aligned}
\frac{\partial}{\partial u_i} l(\beta,\vec{u}|y_{ij}) &= y_{i1}(1-\pi_{i1})+(1-y_{i1})\pi_{i1}+y_{i2}(1-\pi_{i2})+(1-y_{i2})\pi_{i2},
\end{aligned}
\end{equation}
pois $\frac{\partial}{\partial u_i}\pi_{ij}=\pi_{ij}(1-\pi_{ij})$.

Seja $\widehat{u}_i$ o estimador de máxima verossimilhança de $u_i$, temos então que $\widehat{u}_i$ é tal que:

$$
\frac{\partial}{\partial u_i} l(\beta,\widehat{\vec{u}}|y_{ij}) =  y_{i1}(1-\pi_{i1})-(1-y_{i1})\pi_{i1}+y_{i2}(1-\pi_{i2})-(1-y_{i2})\pi_{i2}=0,
$$

Ou seja:

\begin{equation}
\label{log_like_ex6_aux1}
\begin{aligned}
y_{i1}(1-\pi_{i1})-(1-y_{i1})\pi_{i1}=-y_{i2}(1-\pi_{i2})+(1-y_{i2})\pi_{i2}
\end{aligned}
\end{equation}

Pela igualdade acima, se $y_{i1}=y_{i2}$ é fácil ver que não há estimador de máxima verossimilhança para $u_i$, pois precisariamos que $\pi_{i1}=\pi_{j}=y_{i1}$, logo, se $y_{i1}=1$ e $\beta \neq 0$, precisaríamos que $u_i=+\infty$, e, se $y_{i1}=0$, precisaríamos ue $u_i=-\infty$. Se $y_{i1}\neq y_{i2}$, então é fácil ver que $u_i$ é ótimo quando $1=\pi_{i1}+\pi_{i2}$, i.e., quando $\pi_{i2}=1-\pi_{i1}$. Com estas observações, fica fácil ver que apenas as observações onde $y_{i1}\neq y_{i2}$ contribuirão para a verossimilhança de $\beta$ (ficará mais evidente na equação \ref{log_like_ex6_sol1_beta}).

Derivando com relação a $\beta$ obtemos:

\begin{equation}
\label{log_like_ex6_part1_beta}
\begin{aligned}
\frac{\partial}{\partial \beta} l(\beta,\vec{u}|y_{ij}) &= \sum_{i} x_{i1}\left(y_{i1}(1-\pi_{i1})-(1-y_{i1})\pi_{i1}\right)+ x_{i2}\left(y_{i2}(1-\pi_{i2})-(1-y_{i2})\pi_{i2}\right).
\end{aligned}
\end{equation}

Lembrando que $x_{ij}=1$ se, e somente se, $j=2$, temos que:

\begin{equation}
\label{log_like_ex6_part2_beta}
\begin{aligned}
\frac{\partial}{\partial \beta} l(\beta,\vec{u}|y_{ij}) &= \sum_{i} y_{i2}(1-\pi_{i2})-(1-y_{i2})\pi_{i2}
&= \sum_{i} y_{i2}-y_{i2}\pi_{i2}-\pi_{i2}+y_{i2}\pi_{i2}\\
&= \sum_{i} y_{i2}-\pi_{i2}.
\end{aligned}
\end{equation}

Se $\widehat{\beta}$ é o estimador de máxima verossimilhança de $\beta$, então vale que $\frac{\partial}{\partial \beta} l(\widehat{\beta},\vec{u}|y_{ij})=0$, portanto:

\begin{equation}
\label{log_like_ex6_sol1_beta}
\begin{aligned}
\sum_{i} y_{i2}=\sum_{i} \pi_{i2}.
\end{aligned}
\end{equation}

Pela equação \ref{log_like_ex6_aux1}, nos casos onde $y_{i1}=y_{i2}$, podemos considerar $y_{ij}=\pi_{ij}$, logo \ref{log_like_ex6_sol1_beta} pode ser escrita como:

\begin{equation}
\label{log_like_ex6_sol2_beta}
\begin{aligned}
\sum_{y_{i1}\neq y_{i2}} y_{i2}=\sum_{y_{i1}\neq y_{i1}} \pi_{i2} = \exp\{\widehat{\beta}\}\sum_{y_{i1}\neq y_{i2}} \pi_{i1} .
\end{aligned}
\end{equation}

Logo, temos que:

\begin{equation}
\label{log_like_ex6_sol3_beta}
\begin{aligned}
\widehat{\beta}=\ln \left( \frac{\sum_{y_{i1}\neq y_{i2}} y_{i2}}{\sum_{y_{i1}\neq y_{i2}} \pi_{i1} }\right).
\end{aligned}
\end{equation}

Por último, veja que, derivando com relação a $\alpha$ temos que:

\begin{equation}
\label{log_like_ex6_part1_alpha}
\begin{aligned}
\frac{\partial}{\partial \alpha} l(\alpha,\beta,\vec{u}|y_{ij}) &= \sum_{i} y_{i1}(1-\pi_{i1})-(1-y_{i1})\pi_{i1}+y_{i2}(1-\pi_{i2})-(1-y_{i2})\pi_{i2}\\
&= \sum_{i} y_{i1}-y_{i1}\pi_{i1}-\pi_{i1}+y_{i1}\pi_{i1}+y_{i2}-y_{i2}\pi_{i2})-\pi_{i2}+y_{i2}\pi_{i2}\\
&= \sum_{i} y_{i1}-\pi_{i1}+y_{i2}-\pi_{i2}.
\end{aligned}
\end{equation}

Levando em consideração a equação \ref{log_like_ex6_sol1_beta}, temos que o $\alpha$ que maximiza a verossimilhança é tal que:

\begin{equation}
\label{log_like_ex6_part2_alpha}
\begin{aligned}
\sum_{y_{i1}\neq y_{i2}} y_{i1}=\sum_{iy_{i1}\neq y_{i2}}\pi_{i1}.
\end{aligned}
\end{equation}

Daí obtemos que:

\begin{equation}
\label{log_like_ex6_sol3_beta}
\begin{aligned}
\widehat{\beta}=\ln \left( \frac{\sum_{y_{i1}\neq y_{i2}} y_{i2}}{\sum_{y_{i1}\neq y_{i2}} y_{i1} }\right).
\end{aligned}
\end{equation}

Esta fórmula está diferente da do livro (está faltando um $2$ multiplicando o logarítimo), porém não consegui erros nas contas.

# Questão 7

Basta ver que, pela Lei Forte dos Grandes Números, com probabilidade $1$:

\begin{equation}
\label{large_num_implic}
\begin{aligned}
\frac{N_{10}}{N_{10}}\rightarrow \frac{\mathbb{E}[N_{10}]}{\mathbb{E}[N_{10}]}&= \frac{\sum \P(y_{i1}=1,y_{i2}=0)}{\sum \P(y_{i1}=0,y_{i2}=1)}\\
&= \frac{\sum \frac{e^{-\alpha-\mu_i}}{1+e^{-\alpha-\mu_i}}\frac{1}{1+e^{-\alpha-\mu_i-\beta}}}{\sum \frac{1}{1+e^{-\alpha-\mu_i}}\frac{e^{-\alpha-\mu_i-\beta}}{1+e^{-\alpha-\mu_i-\beta}}}\\
&= e^{\beta} \frac{\sum \frac{e^{-\alpha-\mu_i}}{1+e^{-\alpha-\mu_i}}\frac{1}{1+e^{-\alpha-\mu_i-\beta}}}{\sum \frac{1}{1+e^{-\alpha-\mu_i}}\frac{e^{-\alpha-\mu_i}}{1+e^{-\alpha-\mu_i-\beta}}}\\
&= e^{\beta}.
\end{aligned}
\end{equation}

Daí, com probabilidade $1$:

\begin{equation}
\label{large_num_implic}
\begin{aligned}
2\ln \left( \frac{\sum_{y_{i1}\neq y_{i2}} y_{i2}}{\sum_{y_{i1}\neq y_{i2}} y_{i1} }\right)\rightarrow 2\beta.
\end{aligned}
\end{equation}

\pagebreak

# Referências
